sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-07"
x <- collect(parquetFile(sqlContext, parquet_dir))
x <- collect(parquetFile(sqlContext, parquet_dir))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-07"
x <- collect(parquetFile(sqlContext, parquet_dir))
?sparkEnvir
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkEnvir = c("AWS_ACCESS_KEY", "AWS_SECRET_KEY"),
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- strftime("s3n://%s:%s@data-pd-prod/serverlogs_parquet/2016-01-07",
Sys.getenv("AWS_ACCESS_KEY"), Sys.getenv("AWS_SECRET_KEY"))
x <- collect(parquetFile(sqlContext, parquet_dir))
Sys.getenv("AWS_ACCESS_KEY")
Sys.getenv()
Sys.setenv(AWS_ACCESS_KEY = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_KEY = "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")
sparkR.stop()
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sc <- sparkR.init(master="local[*]",
sparkEnvir = c("AWS_ACCESS_KEY", "AWS_SECRET_KEY"),
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- strftime("s3n://%s:%s@data-pd-prod/serverlogs_parquet/2016-01-07",
Sys.getenv("AWS_ACCESS_KEY"), Sys.getenv("AWS_SECRET_KEY"))
x <- collect(parquetFile(sqlContext, parquet_dir))
sparkR.stop()
sparkR.stop()
sparkR.stop()
#initialize sparkR
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- strftime("s3n://%s:%s@data-pd-prod/serverlogs_parquet/2016-01-07",
Sys.getenv("AWS_ACCESS_KEY"), Sys.getenv("AWS_SECRET_KEY"))
x <- collect(parquetFile(sqlContext, parquet_dir))
parquet_dir
parquet_dir <- strftime("s3n://%s:%s@data-pd-prod/serverlogs_parquet/2016-01-07",
Sys.getenv("AWS_ACCESS_KEY"), Sys.getenv("AWS_SECRET_KEY"))
parquet_dir
parquet_dir <- sprintf("s3n://%s:%s@data-pd-prod/serverlogs_parquet/2016-01-07",
Sys.getenv("AWS_ACCESS_KEY"), Sys.getenv("AWS_SECRET_KEY"))
x <- collect(parquetFile(sqlContext, parquet_dir))
parquet_dir <- sprintf("s3n://%s:%s@data-pd-prod/serverlogs_parquet/2016-01-07/",
Sys.getenv("AWS_ACCESS_KEY"), Sys.getenv("AWS_SECRET_KEY"))
x <- collect(parquetFile(sqlContext, parquet_dir))
Sys.setenv(AWS_ACCESS_KEY_ID = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_ACCESS_KEY = "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")
parquet_dir <- "s3n://%s:%s@data-pd-prod/serverlogs_parquet/2016-01-07/"
x <- collect(parquetFile(sqlContext, parquet_dir))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-07/"
x <- collect(parquetFile(sqlContext, parquet_dir))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-07/"
x <- collect(parquetFile(sqlContext, parquet_dir))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-07.parquet/"
x <- collect(parquetFile(sqlContext, parquet_dir))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-07/*.parquet"
x <- collect(parquetFile(sqlContext, parquet_dir))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-17/*.parquet"
x <- collect(parquetFile(sqlContext, parquet_dir))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-17/*"
x <- collect(parquetFile(sqlContext, parquet_dir))
ds_parquet <- parquetFile(sqlContext, parquet_dir)
clear
Ëš
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/"
ds_parquet <- parquetFile(sqlContext, parquet_dir)
names(ds_parquet)
ds_parquet.take(2)
ls(ds_parquet)
methods(ds_parquet)
head(ds_parquet)
head(ds_parquet)
ds_parquet$querystring
ds_parquet$querystring[,2]
ds_parquet$querystring
printSchema(ds_parquet)
showDF(select(ds_parquet, "student_id"))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015-*"
ds_parquet <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015-*"
df <- parquetFile(sqlContext, parquet_dir)
showDF(select(df, "student_id"))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015-12-12_$folder$"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015-12-12/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY_ID = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_ACCESS_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/*/*.parquet"
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/12/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/12/01/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
showDF(select(df, "student_id"))
sparkR.stop()
#initialize sparkR
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY_ID = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_ACCESS_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
showDF(select(df, "student_id"))
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/*/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/*/*/*"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
showDF(select(df, "student_id"))
names(df)
head(df)
names(df)
df.drop('time_taken_ms')
df.drop('time_taken_ms').collect()
df.drop(df.time_taken_ms)
df[df$student_id != '']
df.registerTempTable("logs")
registerTempTable(df, "logs")
res <- sql(sqlContext, "select student_id, http_method, uri, querystring  from logs where student_id <> ''")
head(res)
#initialize sparkR
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY_ID = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_ACCESS_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
registerTempTable(df, "logs")
res <- sql(sqlContext, "select student_id, http_method, uri, querystring  from logs where student_id <> '' limit 10")
head(res)
#initialize sparkR
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY_ID = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_ACCESS_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
registerTempTable(df, "logs")
res <- sql(sqlContext, "select student_id, http_method, uri, querystring  from logs where student_id <> '' limit 10")
head(res)
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/*/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/01*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/01/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/01/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
registerTempTable(df, "logs")
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015/01/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
registerTempTable(df, "logs")
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015-01-*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2015-01-17/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-17/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-18/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-18/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-18/*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
parquet_dir <- "s3n://data-pd-prod/sqlserver_parquet/ActionLog.parquet"
df <- parquetFile(sqlContext, parquet_dir)
registerTempTable(df, "logs")
res <- sql(sqlContext, "select * from logs")
head(res)
parquet_dir <- "s3n://data-pd-prod/sqlserver_parquet/LoginLog.parquet"
df <- parquetFile(sqlContext, parquet_dir)
registerTempTable(df, "logs")
res <- sql(sqlContext, "select * from logs")
head(res)
res
length(res)
res <- sql(sqlContext, "select * from logs limit 10")
head(res)
head(res)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/2016-01-15.parquet"
df <- parquetFile(sqlContext, parquet_dir)
#initialize sparkR
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY_ID = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_ACCESS_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/ 2016-*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
registerTempTable(df, "logs")
res <- sql(sqlContext, "select * from logs limit 10")
head(res)
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY_ID = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_ACCESS_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/ 2016-01-1*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
df.cache()
spark-serregisterTempTable(df, "logs")
res <- sql(sqlContext, "select * from logs limit 10")
spark-serregisterTempTable(df, "logs")
registerTempTable(df, "logs")
res <- sql(sqlContext, "select * from logs limit 10")
head(res)
names(res)
res$student_id
res <- sql(sqlContext, "select student_id from logs limit 10")
res
res.collect()
length(res)
res <- sql(sqlContext, "select student_id from logs")
res
res[-1]
res
res <- sql(sqlContext, "select student_id from logs where student_id <> ''")
res
printSchema(res)
showDF(res)
printSchema(df)
res <- sql(sqlContext, "select student_id from logs where uri LIKE '%busca%'")
showDF(res)
showDF(res)
#initialize sparkR
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
Sys.setenv(AWS_ACCESS_KEY_ID = "AKIAIPKF2KR6FTUC2KJA")
Sys.setenv(AWS_SECRET_ACCESS_KEY = "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")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
sparkR.stop()
sc <- sparkR.init(master="local[*]",
sparkPackages = "com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0")
sqlContext <- sparkRSQL.init(sc)
#legacy import has space :()
parquet_dir <- "s3n://data-pd-prod/serverlogs_parquet/ 2016-01-1*/*.parquet"
df <- parquetFile(sqlContext, parquet_dir)
df.cache()
registerTempTable(df, "logs")
printSchema(df)
res <- sql(sqlContext, "select student_id from logs where uri LIKE '%busca%'")
showDF(res)
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
library(plyr)
library(dplyr)
library(data.table)
library(RJDBC)
library(ggplot2)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(corrplot)
library(descr)
drv <- JDBC("com.microsoft.sqlserver.jdbc.SQLServerDriver",
"/Users/fagnerfeitosa/development/passeidireto/sqljdbc4-2.0.jar")
conn <- dbConnect(drv, "jdbc:sqlserver://172.25.41.85", "dev", "159123")
sql <- "
select top 200000 fr.SenderId, fr.ReceiverId from PDDb.dbo.FriendConnection as fc
inner join PDDb.dbo.FriendRequest as fr
on fr.Id = fc.FriendRequestId
where
fr.AcceptanceDate is not null
order by fr.AcceptanceDate desc
"
res <- RJDBC::dbGetQuery(conn, sql)
df <- data.frame(res)
senders_count <- count(df, vars=c(df$SenderId), sort = TRUE)
boxplot(senders_count$n, ylim = c(0, 10))
length(senders_count$n)
length(senders_count[senders_count$n == 4])
quantile(senders_count$n)
#mean
#median
#quantile
# quartis
# histograma
hist(students$UploadCount, col="red")
# curva densidade
plot(density(students$UploadCount))
# correlacoes - dois numericos
# cor(x, y) - cor.test (p-value, etc)
plot(students$Id, students$UploadCount)
ggplot( data = students, aes( students$LastLoginDate, students$UploadCount )) +
geom_line()
names(students)
freq(students$Gender, students$UploadCount)
freq(students$IsMobileRegister, students$UploadCount)
freq(students$CourseName, students$UploadCount)
freq(students$CityName, students$UploadCount)
freq(students$StateName, students$UploadCount)
names(students)
hist(students$UploadCount, col="red")
boxplot(senders_count$n, ylim = c(0, 10))
length(senders_count$n)
length(senders_count[senders_count$n == 4])
quantile(senders_count$n)
sql <- "
select * from PDDb.dbo.Student s
where s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
students_oya <- data.frame(res)
?density
View(senders_count)
names(students_oya)
names(students_oya)
students_oya
res
sql <- "
select top 10 * from PDDb.dbo.Student s
where s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
data <- data.frame(res)
data
names(data)
sql <- "
select top 10 s.Id, s.GoogleSignUp, s.FacebookSignUp from PDDb.dbo.Student s
where s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
data <- data.frame(res)
data
names(data)
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
library(plyr)
library(dplyr)
library(data.table)
library(RJDBC)
library(ggplot2)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(corrplot)
library(descr)
drv <- JDBC("com.microsoft.sqlserver.jdbc.SQLServerDriver",
"/Users/fagnerfeitosa/development/passeidireto/sqljdbc4-2.0.jar")
conn <- dbConnect(drv, "jdbc:sqlserver://172.25.41.85", "dev", "159123")
sql <- "
select top 10 s.Id, s.GoogleSignUp, s.FacebookSignUp from PDDb.dbo.Student s
where s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
data <- data.frame(res)
data
marks <- c(0,20000,40000,60000,80000,100000)
plot(x,y,log="x",yaxt="n",type="l")
axis(2,at=marks,labels=marks)
names(data)
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
library(plyr)
library(dplyr)
library(data.table)
library(RJDBC)
library(ggplot2)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(corrplot)
library(descr)
drv <- JDBC("com.microsoft.sqlserver.jdbc.SQLServerDriver",
"/Users/fagnerfeitosa/development/passeidireto/sqljdbc4-2.0.jar")
conn <- dbConnect(drv, "jdbc:sqlserver://172.25.41.85", "dev", "159123")
sql <- "
select top 10 s.Id, s.GoogleSignUp, s.FacebookSignUp from PDDb.dbo.Student s
where s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
data <- data.frame(res)
res <- RJDBC::dbGetQuery(conn, sql)
res <- RJDBC::dbGetQuery(conn, sql)
sql <- "
SELECT
TOP 10 s.Id, s.GoogleSignUp, s.FacebookSignUp
FROM PDDb.dbo.Student s
WHERE
s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
data <- data.frame(res)
res <- RJDBC::dbGetQuery(conn, sql)
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
library(plyr)
library(dplyr)
library(data.table)
library(RJDBC)
library(ggplot2)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(corrplot)
library(descr)
drv <- JDBC("com.microsoft.sqlserver.jdbc.SQLServerDriver",
"/Users/fagnerfeitosa/development/passeidireto/sqljdbc4-2.0.jar")
conn <- dbConnect(drv, "jdbc:sqlserver://172.25.41.85", "dev", "159123")
sql <- "
SELECT
TOP 10 s.Id, s.GoogleSignUp, s.FacebookSignUp
FROM PDDb.dbo.Student s
WHERE
s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
data <- data.frame(res)
conn <- dbConnect(drv, "jdbc:sqlserver://172.25.41.85", "dev", "159123")
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
library(data.table)
library(RJDBC)
drv <- JDBC("com.microsoft.sqlserver.jdbc.SQLServerDriver",
"/Users/fagnerfeitosa/development/passeidireto/sqljdbc4-2.0.jar")
conn <- dbConnect(drv, "jdbc:sqlserver://172.25.41.85", "dev", "159123")
sql <- "
SELECT
TOP 10 s.Id, s.GoogleSignUp, s.FacebookSignUp
FROM PDDb.dbo.Student s
WHERE
s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
data <- data.frame(res)
conn <- dbConnect(drv, "jdbc:sqlserver://172.25.41.85", "dev", "159123")
Sys.setenv(SPARK_HOME = "/usr/local/spark-1.5.2/")
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
library(data.table)
library(RJDBC)
drv <- JDBC("com.microsoft.sqlserver.jdbc.SQLServerDriver",
"/Users/fagnerfeitosa/development/passeidireto/sqljdbc4-2.0.jar")
conn <- dbConnect(drv, "jdbc:sqlserver://172.25.41.85", "dev", "159123")
sql <- "
SELECT
TOP 10 s.Id, s.GoogleSignUp, s.FacebookSignUp
FROM PDDb.dbo.Student s
WHERE
s.RegisteredDate > dateadd(m, -12, getdate())
"
res <- RJDBC::dbGetQuery(conn, sql)
data <- data.frame(res)
library(ggplot2)
library(dplyr)
source("tse_file_loader.R")
setwd("~/development/projects/elegibilidade-candidato")
library(ggplot2)
library(dplyr)
source("tse_file_loader.R")
conflicts()
library(ggplot2)
library(dplyr)
source("tse_file_loader.R")
conflicts()
